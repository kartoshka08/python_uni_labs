Обзор методов и алгоритмов полнотекстового поиска.
Новосибирский государственный университет
Адаманский Антон
В предлагаемом обзоре будут рассмотрены основные подходы и методы, применяемые
для решения ряда задач информационного поиска по текстовым данным. Данный обзор не
призван охватить и рассмотреть абсолютное большинство задач информационного
поиска, однако, как надеется автор, он будет полезен в качестве общего освещения
методов поиска, применяемых в современных информационно-поисковых системах.
2

Информационно-поисковые системы.
Один и первых фундаментальных обзоров задач информационного поиска и
информационно-поисковых систем был представлен в работе Рейзенберга, Information
Retrieval” [1], и не смотря на то, что данная работа была опубликована в 1979 году, это до
сих пор один из полных и исчерпывающих обзоров по информационному поиску. В ниже
приведённой классификации полнотекстовых поисковых информационных систем, автор
будет руководствоваться упомянутой работой Рейзенберга. Существующие
информационные системы, работающие с электронными тестовыми документами, можно
условно разделить на две категории: информационно-поисковые системы (в зарубежной
терминологии они фигурирую под термином information retrieval systems), и системы
выборки данных (data retrieval systems). Стоит отметить, что данная классификация
условна и в её контексте многие современные информационные системы совмещают в
себе свойства, как систем выборки данных так и информационно-поисковых систем.
Примем в дальнейшем следующие обозначения: ИПС и СВД соответственно для
информационно-поисковых систем и систем выборки данных. Базовые отличия систем
выборки данных и информационно-поисковых систем, представленных в таблице 1:
СВД ИПС
соответствие данных
поисковому запросу
точное частичное
классификация документов детерминированная вероятностная
язык запросов искусственный естественный
критерии выборки
документов
булева функция
релевантности
вероятностная функция
релевантности
устойчивость к ошибкам в
данных и запросах неустойчивы устойчивы
Таблица 1.
3
Типичным примером систем выборки данных являются классические реляционные СУБД.
Где в качестве языка запросов используется тот или иной диалект языка запросов SQL.
Этот язык искусствен и позволяет задавать поисковые запросы лишь для поиска на точное
соответствие или поиска по заданному шаблону. В современных системах выборки
данных основной задачей является обеспечение надёжного и эффективного хранения
данных, а также высокой скорости выполнения поисковых запросов пользователей. В
свою очередь информационно-поисковые системы предназначены для решения более
общей задачи поиска, чем поиск на точное соответствие, и где конечной целью поиска
является выбор релевантной поисковому запросу информации, степень релевантности
которой можно определить как степень её смысловой близости к поисковому запросу, а
это в свою очередь ведёт к тому что поисковые запросы в такого рода системах должны
быть основаны на естественном языке, т.е на том же языке в котором сформулирована
исходная информация. И системы выборки данных и информационно поисковые системы,
работают с некоторой коллекцией документов. Исходную коллекцию документов можно
рассматривать как список записей (документов), где каждая запись содержит в себе
некоторый список слов, состоящих из символов алфавита. В реальных информационных
системах в исходном множестве документов может содержаться дополнительная
информация, описывающая документы, которая так же может использоваться, для
осуществления поиска. Решающую роль при разработке современных ИПС играют
объёмы исходных данных, поскольку к системам работающих на сравнительно больших
объёмах данных, а к таким системам можно отнести, как системы Web поиска, так и
крупные электронные библиотеки, применяются всё более жёсткие требования к
производительности и качеству поиска. Поскольку в настоявшее время количество
текстовой информации, представленной в электронном виде, продолжает и по всей
видимости будет возрастать экспоненциально, вопрос реализации эффективных,
масштабируемых и производительных информационно-поисковых систем на
сегодняшний день остаётся открытым для исследователей. Ведь только в Интернете в
настоящее время, количество доступной и содержательной информации по оценкам
аналитиков составляет порядка в 4,200 терабайт [2], для примера в базах данных
Национального Климатического Центра и NASA (США) объёмы содержательной
информации превышают 600 терабайт.
4
Введём некоторые базовые определения:
Определение 1. Назовем алфавитом, конечное множество символов { } α α α k Α = τ , 1 , 2 ,K ,
где τ - пробельный символ, Α = k , и k > 0 - количество символов алфавита.
Определение 2. Строкой длинны n , будем называть последовательность символов
T = { } t1 ,t2 ,Kt n ,$ , где ∀i, , ti ∈ Α и $ - специальный символ не принадлежащий алфавиту и
обозначающий конец строки.
Определение 3. Строку w , не содержащую пробельного символа τ , назовём словом и
будем считать что множество слов W всегда конечно.
Определение 4. Поисковым шаблоном P = {p1, p2 K pm } назовём строку состоящую из
непустого конечного множества слов, разделенных пробельным символом τ . В данном
случае P = m - длина шаблона в символах, pi ∈W , где i - номер слова в шаблоне, а W -
множество всех слов. Слова в поисковых шаблонах и документах, будем называть
терминами.
Замечание. Далее по тексту, длинна поискового шаблона P = {p1, p2 K pm } всегда будет
обозначаться символом m , а общую длину исходных данных { }n T t ,t ,Kt = 1 2 , для которых
будет решаться задача поиска обозначим как n .

Типы информационного поиска.
В данном разделе определим основные типы поисковых запросов. Здесь следует отметить,
что перечисленные ниже типы поисковых запросов отображают так же область задач,
решаемых информационно-поисковыми системами.
Булевый поиск
В данном типе информационного поиска, запросы строятся на основе элементарных
терминов документов (слов или словоформ), находящихся между собой в отношениях,
определённых предикатами, такими как дизъюнкция (∨ ), конъюнкция ( ∧ ) и отрицание
(¬ ). Булевые запросы обычно используются в поиске на точное соответствие, в котором
документы проверяются на наличие того или иного термина. Типичную форму булевого
запроса можно представить в виде: ( ) ( ) P = p1 ∧ p2 ∧K∧ pn ∨K∨ p1∧ p2 ∧K∧ pn , где pi -
i -й термин поискового запроса.
5
Поиск по релевантности
В общих чертах задачу релевантного поиска можно сформулировать довольно просто.
Предположим, что имеется некоторое множество электронных текстовых документов и
существует пользователь, который, взаимодействуя с системой и формулируя поисковые
запросы, получает множество удовлетворяющих запросу документов. В этом случае
задачей релевантного поиска является как можно более полная и точная выборка
подмножества документов наиболее близких по смыслу (релевантных) поисковому
запросу. Здесь под понятием близости документа поисковому запросу следует понимать
некоторую функцию корреляции между каждым документом и запросом, вычисление
которой позволяет определить степень близости информационного содержания документа
и запроса. Вид функции релевантности, зависит от конкретной реализации
информационно-поисковых систем. В большинстве ИПС функция релевантности
документа запросу является вероятностной (т.е. о соответствии документа поисковому
запросу можно говорить только с какой-то степенью вероятности) и базируется на
некоторой информации о документах, автоматически получаемой системой в ходе анализа
их информационного контекста документов (вплоть до анализа синтаксиса и семантики).
Также широко применяются и методы статистического анализа текста документов [1]. В
данном контексте основной проблемой информационно-поисковых системах является
автоматическая обработка и классификация документов для последующего вычисления
функции релевантности, где в качестве языка запросов может использоваться язык
запросов приближённый к естественному. Типичными представителями поисковых
систем, осуществляющих релевантный поиск, являются системы Web поиска: Google,
Яндекс, AstaVista. В таких системах, сначала осуществляется обычный булевый поиск
(который может учитывать и грамматические формы терминов запроса) и на основе его
результатов, производится поиск документов, которые имеют ссылки на документы из
базового множества и наоборот. Например Google [3], а так же многие другие ИПС
вычисляют степень релевантности документов, используя для этого анализ ссылок
содержащихся в документах. Более подробную информацию о методиках определения
релевантных документов в поисковых системах Интернета можно найти в [4], [5], [3].
Эффективность работы информационно-поисковых систем, использующих релевантный
поиск удобно измерять такими характеристиками как “точностью” (precision) поиска,
определяемой как отношение полученных в результате поиска релевантных запросу
документов ко всем выбранным в ходе поиска документам. Другая величина,
характеризующая эффективность поиска: “объём выборки” (recall), определённой как
отношение выбранных в результате запроса релевантных документов ко всем
6
релевантным документам, содержащихся в ИПС. Данные характеристики эффективности
поиска дополняют друг друга и отметим, что эти две величины находятся в обратнопропорциональной зависимости друг относительно друга, т.е. чем больше будет точность
выборки получаемой в результате поиска, тем меньший объём релевантных документов
будет на выходе системы и наоборот.
Поиск по сходству
Представляет собой модификацию булевого поиска, где поисковой системой учитываются
возможные неточности в задании поисковых терминов или в электронном представлении
документов. С одной стороны поиск по сходству увеличивает объём выборки
релевантных запросу документов, однако так же увеличивается и вероятность выборки
документов реальная степень релевантности которых очень мала. Поиск по сходству
может быть очень полезен при выборке информации в специализированных электронных
библиотеках таких как медицинские базы данных, или в системах распознавания текста,
где существует определённая вероятность неточного задания термина в поисковом
запросе или же в самом документе. Меру корреляции между терминами поискового
запроса и терминами исходных документов обычно определяют в виде расстояния
редактирования [7], [17] наиболее простое и широко-распространенное определение
расстояния редактирования задаётся функцией Левенштайна.
Определение 5. Функция Левенштайна равна минимальному количеству операций
редактирования, таких как вставка, замена, удаление, необходимых для преобразования
слова w в слово v .
Ясно, что поиск по сходству можно свести к булевому поиску, если положить пороговое
расстояние редактирования между терминами запроса и терминами документов равным
нулю. Более подробную информацию о поиске по сходству можно найти в [16], [17], а
также на русскоязычном Интернет сайте Леонида Бойцова, посвящённому методам
нечёткого поиска [8].
7
Методы информационного поиска.
В настоящее время наиболее распространены поисковые системы, использующие два
базовых принципа информационного поиска: поиск по ключевым словам (терминам)
документов и поиск на основе кластерных методов и векторных моделей. [1], [9]. Однако
многие информационно-поисковые системы используют кластерные методы в качестве
дополнения к поиску по ключевым словам [10], [11] что позволяет существенно повысить
эффективность и точность информационного поиска, а также удобочитаемость
результатов поиска. К системам, использующим поиск по ключевым словам можно
отнести большинство широко распространенных поисковых систем, среди них такие
системы Web поиска как: Яндекс, Google, AstaVista, Yahoo. В подобных системах процесс
поиска сводится к поиску во множестве документов вхождений каждого из заданных
ключевых слов, с последующей сортировкой этих документов по степени релевантности.
Рассмотрим методы поиска по ключевым словам более подробно.
Последовательный поиск
В данном разделе будет рассмотрено несколько методов последовательного поиска по
ключевым терминам документов. Для систем осуществляющих последовательный поиск,
поисковым индексом являются сам набор исходных документов. И на сравнительно
небольших объёмах данных, методы последовательного поиска являются наиболее
быстрыми, хотя скорость поиска у такого рода систем жёстко ограничена и по
определению и будет всегда не лучше чем O(n), где n - суммарный размер исходных
документов. Однако благодаря простоте реализации, и высокой эффективности на
небольших объёмах данных, алгоритмы последовательного поиска нашли свою нишу и
широкое применение. Сканирующие алгоритмы полнотекстового поиска могут
применять и для решения задач нечёткого поиска где при сканировании вычисляется
функция близости между терминами запроса и терминами документов. Последовательный
поиск широко применяется во многих небольших поисковых системах и в настоявшее
время существует множество работ и алгоритмов использующих идею метода
сканирования словаря для осуществления нечёткого поиска [12], [13]. Задачу поиска
сформулируем следующим образом: пусть существует некоторый текст { }n T t t Kt 1 2 = , и
требуется найти все вхождения в него подстроки (шаблона) P = {p1, p2 K pm }, где m ≤ n .
Простейшее решение задачи состоит в последовательном сравнении и заключается в
следующем: начиная с 1t и 1 p , символов T и P до тех пор, пока не будет обнаружено
8
совпадение или несовпадение. В последнем случае следует вернуться к началу сравнения
1t , сдвинуться на один символ по тексту т.е. до 2t и повторить попытку. Этот алгоритм
требует выполнения не менее n-m+1 сравнений и работает медленно. Вместе с тем он
весьма прост в реализации и позволяет проводить поиск с использованием шаблонов
любой длинны.
Точный поиск по алгоритму Бойера-Мура.
Известно несколько способов улучшения описанного выше простейшего решения
последовательного поиска. Наиболее известны два из них: алгоритм Бойера-Мура [14] и
алгоритм Кнута-Морриса-Пратта [18]. В большинстве случаев при решении задач точного
поиска, методом последовательного сканирования алгоритм Бойера-Мура работает
быстрее, и широко применяется в программах редактирования текста, поэтому вкратце
опишем его основную идею. Сравнение терминов происходит с права на лево и
начинается, между последним символом шаблона P и m -им символом текста T , где
P = m - длинна шаблона. Далее, если обнаруживается несовпадение на некоторой паре
терминов pi и j t , соответственно шаблона и текста, тогда определяем в каких позициях
шаблона содержится несовпадающий символ j t , и на основе этого принимаем решение о
сдвиге шаблона относительно текста. Реализация данного алгоритма требует
предобработки поискового шаблона перед началом поиска за время O(m) . И сам поиск
происходит за время ) O(m ⋅ n , но в лучшем случае поиск алгоритмом Бойера-Мура может
производиться за время ) O(n / m . Полное описание работы данного алгоритма можно
найти в оригинальной публикации [14] .
Стоит отметить ещё один сканирующий алгоритм поиска по тексту, так называемый
алгоритм СДВИГ-И. Его опубликовали [19] в 1991 году два исследователя из Аризонского
университета и Bell Labs - Уди Манбер (Udi Manber) и Сан Ву (Sun Wu). Данный алгоритм
так же используется в свободно-распространяемой утилите полнотекстового поиска agrep
[13] .
Основные характеристики алгоритма СДВИГ-И:
• Предобработка поискового шаблона P за время O(m) .
• Время поиска порядка O(n) , не зависимо от размера алфавита и длины поискового
шаблона P .
9
• Существуют очень эффективная реализация данного алгоритма, если длинна
шаблона P будет умещаться в машинное слово вычислительной машины.
• Легко расширяем для решения задач нечеткого поиска.
Затронутые выше сканирующие алгоритмы поиска являются лишь малой частью среди
всего многообразия подобных алгоритмов, и более подробное описание рассмотренных
выше алгоритмов можно найти здесь [6] .
Информационно-поисковые системы строящие поисковый индекс.
Большинство современных информационно-поисковых систем, для осуществления поиска
строят на основе исходной информации, логические и физические структуры данных,
представляющие собой поисковый индекс, который позволяет реализовать некоторую
заданную модель информационного поиска. Преобразование информации в
информационно-поисковых системах строящих поисковый индекс, обычно состоит из
следующих базовых этапов:
1) Анализ данных исходного множества текстовых документов и их преобразование в
вид, удобный для построения полнотекстового индекса вычислительной машиной,
выделение из документов содержательной информационной основы.
2) Анализ данных полученных в пункте 1. и последующее построение поискового
индекса. В данном случае индекс является представлением данных, логическая
модель которого определяет способ обработки и интерпретации данных и
позволяет осуществлять информационный поиск.
3) Преобразование поисковых запросов в формат позволяющий использовать
поисковый индекс для вычисления функции релевантность запросов и документов
и выборки релевантных запросу документов.
При обработке информации и построении поискового индекса потенциально можно
использоваться достаточно широкий спектр методов анализа текстовой информации
документов, как например методы статистического, семантического, синтаксического и
лингвистического анализа текста. Однако методы анализирующие семантику и синтаксис
текстовой информации [21], [22] вплоть до настоящего времени не получили широкого
распространения ввиду своей сложности и относительно низкой эффективности.
Наибольшее распространение получили методы, использующие статистический анализ
документов. Статический анализ уже давно получил широкое применение в задачах
10
информационного поиска. Впервые этот подход был рассмотрен в работах Луна [20], где в
качестве меры значимости отдельных фраз, являлась частота их появления в документах.
И при этом наиболее значимые слова находятся в середине частотного диапазона их
появления в документах, т.е. значимые слова не входят не входят в число ни наиболее
часто, ни наиболее редко встречающихся слов. В настоящее время эти простые идеи Луна
лежат в основе большинства современных информационно-поисковых систем. Определим
исходное представление любого документа как список слов с учётом порядка их
следования в документе. Процесс выделения значимого информационного содержимого
документов обычно состоит из следующих шагов:
1) Удаление очень часто и редко встречающихся слов
2) Выделение общих языковых словоформ, слов текста документов.
3) Определение эквивалентных (синонимичных) словоформ, т.е. построение
тезауруса.
В ходе данного процесса, текстовая информация документов нормализуется в список
ключевых словоформ, которые служат для классификации документов в ходе поисковых
запросов и используются для построения текстового индекса документов. Стоит отметить,
что на этапе преобразования исходной текстовой информации в поисковый индекс при
использовании стемминга [24] текста, некоторая часть содержательной информации
безвозвратно теряется, поскольку при стемминге, к одной словоформе могут быть
приведены два и более разных по семантике слова [23], [24], однако даже при
использованием чисто алгоритмических методов стемминга, удаётся получить
удовлетворительную точность поиска.
Хеширующие методы.
Идея заключающаяся в основе хеширующих методов поиска состоит в следующем: пусть
на множестве ключевых терминов документов задана функция (хеш-функция) ) ( H wi ,
отображающая множество исходных терминов документов в конечный отрезок целых
чисел [ ] nKm , и пусть n ≤ m . Выделим множество ячеек памяти для m − n +1 терминов и
для каждого термина wi будем помещать его в ячейку под номером ) ( H wi . Если
( ) ( ) H wi ≠ H wj для всех i ≠ j , то для поиска произвольного термина wi потребовалось бы
только одно обращение к элементу массива под номером ) ( H wi . Но поскольку
построение хеш-функции на множестве терминов довольно сложная задача, на практике
допускаются коллизии хеширования когда ) ( ) ( H wi = H wj , для некоторых пар i ≠ j .
Существую два способа разрешения таких коллизий:
11
1. При хешировании нового термина, если возникает коллизия, то помещаем термин в
ближайшую свободную ячейку или ячейку с номером, вычисляемым по
некоторому правилу.
2. Хранить в ячейке ссылку на список терминов с одинаковыми значениями хешфункции.
Простота реализации, и высокая скорость поиска на достаточно небольших объёмах
данных, являются основными преимуществами хеширующих методов поиска.
Инвертированные файлы.
Логически, инвертированные файлы устроены подобно указателю в конце книги. А точнее
в виде списка ключевых терминов, где каждому термину сопоставлен список его
вхождений в документы. Каждое вхождение ключевого термина, определяется ссылкой на
исходный документ содержащий данное ключевое слово. Инвертирование является
полным, если в списке вхождений терминов помимо ссылок на документы содержится
информация о точном местоположении каждого термина, однако в этом случае размер
индекса может быть очень велик, и в несколько раз превышать суммарный размер
исходных документов. Поэтому в большинстве систем, термины в инвертированном
списке ссылаются на документы как таковые, где не указывается точное местоположение
термина в документе. Поиск происходит посредством сканирования инвертированного
списка ключевых слов для нахождения соответствующих терминов запроса и документов,
и последующего объединения или пересечения результатов полученных для каждого
термина поискового запроса. Для уменьшения размеров инвертированного индекса
обычно применяют неполное инвертирование, когда вместо ссылок на точный адрес
термина, хранятся только ссылки на документы или на блоки данных некоторого
фиксированного размера в которых содержится данный термин. Например блочная
адресация в инвертированном списке, применяется в широко распространённой
программе полнотекстового поиска как glimpse [12], где сначала с помощью ИФ
вычисляются блоки данных, содержащие все заданные запросом термины, которые затем
последовательно сканируются на более точное соответствие поисковому запросу. В
современных поисковых системах, основанных на ИФ применяются также методы сжатия
инвертированного индекса и в настоящее время размер индекса сжатых инвертированных
файлов может составлять лишь 4-10% [12] от общего объёма исходных документов. Для
ускорения поиска инвертированный словарь ключевых слов документов может быть
представлен в виде дерева или хеш-таблицы. В системах работающих с большим объёмом
12
исходных текстовых данных инвертированные файлы организованы в виде иерархических
структур.
Сигнатурные файлы.
Идея сигнатурных файлов была разработана в качестве альтернативы инвертированным
файлам, поскольку долгое время не удавалось обеспечить компактность их индекса.
Рассмотрим структуру сигнатурных файлов более подробно. Пусть существует хешфункция f (w) отображающая множество ключевых слов во множество целых чисел,
допустим от 1 до n . Далее, каждому документу ставим в соответствие битовый вектор,
где i компонент вектора равен 1 тогда и только тогда, когда в документе существует
ключевое слово wk такое что f w i ( , k ) = назовём такой вектор сигнатурой документа.
Собственно сигнатурный файл является списком сигнатур всех актуальных документов
ИПС. Процесс поиска происходит следующим образом: для каждого слова w в поисковом
запросе выбираются только те сигнатурные векторы документов, в которых единица стоит
в i позиции, где f (w) = i , поскольку очевидно, что документы только c такими
сигнатурами могут содержать ключевое слово и далее только эти документы сканируются
на наличие ключевого слова. Основным недостатком данного метода является
сканирование при поиске списка всех сигнатур документов. Для преодоления этого
ограничения используется метод битовых срезок. Суть метода битовых срезок
заключается в построении сигнатуры, позволяющей делать обратное отображение
терминов в документы их содержащие. В данном случае строится вектор длины n, где n
определяется верхней границей определённой выше хеш-функции и для каждого элемента
i данного вектора создаётся битовый вектор длины M , где M – количество исходных
документов, в котором единицы соответствуют номерам тех документов, которые
содержат хотя бы одно слово w удовлетворяющее условию f ( . w) = i Поиск по битовым
срезкам происходит следующим образом: для всех ключевых слов запроса вычисляются
значения хеш-функции ) f (w , и далее по этим значениям выбираются соответствующие
битовые срезки, которые соединяются между собой с помощь конъюнкции или
дизъюнкции (в зависимости от того в каком отношении термины поискового запроса, а
они могут соединяться через OR или AND) и выбирается множество документовкандидатов возможно удовлетворяющие запросу. Основное влияние на точность такого
рода поиска определяет вид хеш-функции ) f (w . Ясно, что при большом количестве
документов размер битовой срезки может быть очень велик, поэтому применяют метод
блочной битовой срезки, в которой i элемент срезки соответствует не одному документу, а
13
целому блоку текстовой информации, размер которого задаётся при создании индекса.
Несмотря на то, что СФ были придуманы, как альтернатива инвертированным файлам, с
целью уменьшения размер индекса, однако как показало сравнение [26] в настоящее время
СФ существенно уступают сжатым ИФ по нескольким параметрам:
• Сжатые ИФ занимают меньше места на диске 5-10%, против 30-40% у СФ.
• СФ в отличие от ИФ очень чувствительны к выбору параметров индексации, и
неудачный их выбор достаточно сильно влияет на размер индекса,
производительность и эффективность поиска. [12]
• Скорость как булевого, так и ранжированного поиска в ИФ выше чем у СФ при тех
же объёмах данных. Подробные данные о сравнительном измерении времени
поиска СФ и ИФ приведены в [26].
• Индекс для ИФ создаётся, существенно быстрее чем в случае СФ.
Суффиксные массивы
Хотя инвертированные файлы в настоящее время очень широко используются, к
сожалению они не позволяют производить поиск по произвольной подстроке - поиск в ИФ
может производиться лишь с точностью до некоторого ключевого слова. К тому же в
большинстве индексов, основанных на ИФ с целью экономии дискового пространства
применяют неполное инвертирование, но неполное инвертирование ведёт к увеличению
вычислительных затрат например на поиск по фразам, где необходимо определять точное
положение ключевых слов в документах. Более того, довольно проблематично с помощью
ИФ индексировать текст например на японском или китайском языке, поскольку в данных
языках не существует чёткого определения слова. По той же причине ИФ также не могут
осуществлять поиск в последовательностях ДНК. Для решения данных проблем была
придумана структура данных [27], позволяющая производить более детализованные
запросы, и так же не требующая деления исходных данных на слова или словоформы. В
одних источниках эта структура данных фигурирует как суффиксный массив (suffix array),
в других как PAT массив (PAT array). В данном обзоре я буду придерживаться первого
варианта. Итак, неформально суффиксный массив это отсортированный массив всех
суффиксов некоторой строки, который позволяет производить дихотомический поиск,
проиллюстрируем данную структуру данных на примере, допустим для строки
S = A B A A B B A B B A C, построим список всех суффиксов для этой строки, он показан на
таблице 1, где i -й суффикс начинается с i -ого символа строки.
14
индекс суффикс
0 A B A A B B A B B A C
1 B A A B B A B B A C
2 A A B B A B B A C
3 A B B A B B A C
4 B B A B B A C
5 B A B B A C
6 A B B A C
7 B B A C
8 ВАС
9 АС
10 С
Таблица 1.
Далее все суффиксы данного массива лексиграфически сортируются, при этом сохраняя
адрес каждого суффикса, в итоге получим отсортированный список суффиксов
в таблице 2.
индекс суффикс
2 A A B B A B B A C
0 A B A A B B A B B A C
3 A B B A B B A C
6 A B B A C
9 АС
1 B A A B B A B B A C
5 B A B B A C
8 ВАС
4 B B A B B A C
7 B B A C
10 С
Таблица 2.
Замечание. Суффиксы в таблицах 1 и 2 выписаны только для наглядности, поскольку
доступ к каждому суффиксу происходит с помощью индексов, так как по определению,
индекс суффикса равен позиции его первого символа от начала исходной строки.
Тогда суффиксным массивом для данной строки S будет следующий набор, индексов
суффиксов: SSA = [2, 0, 3, 6, 9, 1, 5, 8, 4, 7, 10]. Нетрудно видеть, что размер суффиксного
массива будет порядка ) O(n , и что данный массив можно использовать для поиска по
строке S за логарифмическое время, поскольку он содержит индексы всех суффиксов S в
15
лексикографическом порядке. В худшем случае количество операций сравнения для
поиска будет порядка ) O(mlog n , однако время поиска может быть улучшено до значения
O(m + log n) путем использования некоторой дополнительной информации о структуре
суффиксного массива, стоит отметить что эффективность данного поиска не зависит от
мощности алфавита. Однако до настоящего времени, остаётся до конца неразрешённой
проблема построения суффиксных массивов, суть которой в проблеме
лексикографической сортировки всех суффиксов произвольной строки. Отмечу так же,
что проблема сортировки суффиксов занимает не последнее место и в алгоритмах сжатия
информации основанных на так называемом преобразовании Барроуза-Уилера (BurrowsWheeler’s transformation) [28]. Потенциально, список отсортированных суффиксов
некоторой строки длинной n , можно получить за ) O(n операций, для этого необходимо
лишь построить на данной строке суффиксное дерево (сама операция построения дерева
может занимать ) O(n операций), затем совершить обход всех узлов дерева и выбрать все
суффиксы строки в уже отсортированном порядке. Однако ввиду довольно большого
объёма памяти занимаемой суффиксными деревьями и сложности реализации данного
алгоритма, подобный метод сортировки суффиксов строки не получил широкого
практического распространения. На сегодняшний день существует множество
разнообразных методов построения суффиксных массивов, и по ним можно бы было
написать отдельный обзор, однако это не входит в цели данной работы, ниже приведены
ссылки на некоторые из наиболее популярных алгоритмов [29] сортировки суффиксов и
построения суффиксных массивов:
• Алгоритм Манбера-Майерса (Manber-Myers) [27], позволяет получить суффиксный
массив за O(nlog n) операций. Минимальная занимаемая память при реализации
данного алгоритма на ЭВМ равна: 13n байт, здесь полагается что размер исходной
строки равен n байт и на каждый индекс, определяющий суффикс строки
отводится 4 байта (int).
• Одноимённый алгоритм Sadakane, японского специалиста по суффиксным
массивам [31], [30] позволяет строить суффиксные массивы в худшем случае за
O(nlog n) операций, однако среднее количество операций может быть оценено как
O(nloglog n). В основе данного алгоритма лежит алгоритм Бентлей и Седжевика
[37]. На практике, данный алгоритм работает существенно быстрее чем алгоритм
Манбера-Майерса. Занимаемая память: 9n байт.
16
По сути вышеприведённые алгоритмы сортировки используют специфические свойства,
набора суффиксов строки, что позволяет добиться времени сортировки порядка ) O(nlog n
и линейного расхода памяти, простой алгоритм быстрой сортировки (QuickSort), не
учитывающий специфических свойств суффиксных массивов обеспечил бы подобную
сортировку лишь за время ) ( log 2 O n n .
Деревья
К сожалению полнотекстовые индексы базирующиеся на структурах данных,
основанных на списках не могут обеспечить время поиска по текстовым данным лучше
чем O(log n) . К тому же списки ключевых терминов, позволяющие осуществлять поиск с
логарифмическим временем, должны быть отсортированы, ведь только тогда по ним
можно осуществлять бинарный поиск за логарифмическое время. Но в свою очередь
отсортированные списки, трудно модифицировать, вставлять и удалять ключевые
термины, количество операций необходимых для этого обычно не меньше чем O(n) .
Однако существуют структуры данных придуманные самой природой, и свободные от
упомянутых выше недостатков – это деревья. Как уже упоминалось выше, бинарные
деревья могут применяться в качестве структуры данных, для инвертированных файлов.
Для этого необходимы бинарные лексиграфические отношения между ключевыми
терминами инвертированного списка, т.е. два любых термина wi и wj должны быть
лексикографически сравнимы, для того чтобы построить бинарное дерево терминов. В
этом случае время поиска по шаблону будет логарифмическим и ограничено величиной
O(m ⋅ log n). Существует масса примеров применения древовидных структур в задачах
полнотекстового поиска, да и не только в них.
Суффиксные деревья
Множество разных задач, начиная от задач поиска наибольшей общей подстроки,
комплиментарных пар в последовательностях ДНК и заканчивая задачами сжатия
информации, нечёткого поиска и кластеризации можно эффективно решить с помощью
суффиксных деревьев. Рассмотрим данную структуру данных более подробно.
Суффиксным деревом T для строки n S s s s Ks = 1 2 3 , назовём структуру данных,
обладающую следующими свойствами:
• Представляет собой дерево с корневым узлом, имеющее ровно n листьев,
пронумерованных от 1 до n
• Каждый внутренний некорневой узел имеет по меньшей мере два потомка.
17
• Каждое ребро дерева, помечено непустой подстрокой строки S и не существует
двух и более рёбер, исходящих из одного узла, помеченных строками
начинающихся с одинаковых символов.
• Для любого листа i дерева T, соединение всех меток рёбер на пути от корня до
листа i образует подстроку i n
i S = s Ks строки S , т.е. суффикс строки S
начинающийся с i -ой позиции.
В качестве примера на рис.1 показано суффиксное дерево для строки S = BANANAS .
рис. 1
Очевидно что данная структура данных обеспечивает поиск за ) O(m + k операций
сравнения, где k − число всех вхождений искомой подстроки в исходной строке. Лучшие
из существующих алгоритмов построения СД, как например алгоритм Эско Укконена
(Esko Ukkonen) [32] строит суффиксное дерево за линейное время O(n) . Суффиксное
дерево построенное для некоторого набора строк {S1S2 S3KSm }, позволяет решить задачу
на нахождение наибольшей общей подстроки данного набора строк. Однако существуют и
слабые стороны данной структуры данных, в частности занимаемая ими память. Её можно
оценить, посчитав количество узлов, необходимых для построения суффиксного дерева
строки длинной n . Поскольку каждый узел дерева имеет как минимум два потомка, и при
этом каждый потомок порождает уникальный путь до одного и n суффиксов строки S , то
это значит что всего внутренних узлов будет не меньше чем n , и прибавим сюда
количество внешних узлов (листьев дерева), получаем в итоге 2n узлов, из этого следует,
что можно построить суффиксное дерево, занимающее ) O(n дискового пространства.
Однако более подробные оценки требуемого места для суффиксного дерева в реальных
системах, менее утешительны, так для текста длинны n , дерево в среднем занимает
порядка 20n байт. Подробнее о суффиксных деревьях вы можете узнать в [33], [34], [35].
18
Тернарные деревья поиска
Стоит отметить метод поиска по набору строк, базирующийся на, так называемых,
тернарных деревьях поиска (ternary search trees). Впервые о тернарных деревьях было
упомянуто ещё в 1964 году [36], однако только относительно недавно идеи лежащие в
основе данной структуры данных получили своё практическое развитие [37]. Пусть для
примера, необходимо осуществить поиск по следующим двенадцати двухбуквенным
строкам:
as at be by he in is it of on or to. Наиболее эффективно по скорости задачу такого поиска
решила бы структура данных в виде дерева представленного на рисунке 2. Где степень
ветвления каждого узла равна A , т.е. равна количеству символов в алфавите.
Рис. 2
Поиск по шаблону происходит очень быстро, начиная с корня дерева и первого символа
шаблона спускаемся по дереву к узлу, соответствующему второму символу шаблона и так
далее. Поиск закончится в одном из двух случаев: либо мы дойдём до последнего символа
шаблона (и это будет значить положительный результат поиска), либо мы уткнёмся в
нулевую ссылку на следующий узел, что будет говорить об отсутствии данного шаблона
среди сходных строк. Однако с практической токи зрения это довольно неудобная
структура данных, поскольку в каждом узле дерева должны храниться ссылки на A
потомков, большинство из которых будут пустующими, и такое дерево будет занимать
очень много места, относительно объёма исходных данных. Для того чтобы разрешить эту
проблему, в качестве компромисса между бинарными деревьями и деревьями со степенью
ветвления A был придуман некоторый их гибрид - тернарные деревья, степень ветвления
которых не более чем 3, позволяющие производить поиск гораздо эффективнее, чем в
бинарных деревьях поиска и быть довольно компактными. Поиск в этом случае будет
выполняться за )) O(m + log(n операций сравнения. На рисунке 3 приведён пример
тернарного дерева для того же набора данных.
19
Рис 3.
Процесс поиска может проходить следующим образом, допустим что мы ищем строку he,
тогда начиная с корня (узла i) сравниваем с узлом первый символ шаблона h и i,
поскольку h > i то переходим в узел b, далее h < b и переход в узел h, поскольку первый
символ шаблона и узел совпадают, переходим к центральному потомку и продолжаем
сравнение но уже со вторым символом шаблона. Алгоритм построения тернарного дерева
аналогичен, рассмотренному выше поиску по дереву. Ниже приведена сравнительная
таблица производительности поиска и построения индекса для тернарных деревьев и для
хеш-таблицы. По сравнению с хеш-таблицами тернарные деревья позволяют очень быстро
вернуть отрицательный результат поиска для шаблона, заведомо не содержащегося в
исходных документах. Стоит отметить, что тернарные деревья позволяют также
эффективно осуществлять поиск по сходству, и уже много лет существует и работает
система автоматического распознавания текста фирмы Bell Labs (OCR System), где для
поиска терминов в словаре английского языка, объемом примерно в 34,000 символов
впервые были использованы тернарные деревья поиска.
Кластерные методы и векторные модели.
Кластерные методы в основе своей позволяют реализовать процесс классификации,
применённый к исходному множеству документов или их составным элементам, и разбить
(классифицировать) документы на группы (кластеры), в которых все элементы по
определённому набору их свойств можно считать однотипными (классифицированными
по признаку). Многие методы классификации элементов, основаны парных отношениях
между элементами, подлежащими классификации, и на их основе можно реализовать
процесс классификации и построить систему кластеров. В качестве примера рассмотрим
метод классификации построенный на основе так называемой векторной модели. Суть
метода достаточно проста, разбиваем все исходные документы на элементарные термины
i t , например на ключевые слова и, перенумеровав все, отличные друг от друга термины,
20
сохраняем их в словаре. Допустим, что всего в словаре n разных терминов. Тогда, каждый
документ Di можно представить в виде вектора i
n
i
k
i i S i
= w1w2Lw Lw размерности n , в
котором только компоненты с номерами i
k i
i
t t 1L ( ) отличны от нуля и равны весам терминов,
а k(i) число уникальных терминов, содержащихся в документе Di . Веса терминов можно
определить следующим образом: пусть i
l f - частота вхождения термина под номером l в
документ Di , а = ∑
i
i
l l F f суммарная частота вхождения данного термина по всем
документам. Тогда вес термина l в документе Di определим как l
i
l
i
wl = f / F . Ясно, что
таким выбором весовых значений терминов мы хотим выделить наиболее характерные
для каждого документа свойства, использовав для этого простой статистический анализ
документов, и это лишь один из способов вычисления такого рода весовых
коэффициентов. Аналогичный вектор весов, можно составить и для поискового запроса.
Теперь ввёдём некоторую, метрику в векторном пространстве размерности n ,
определяющую степень близости (или скорее степень непохожести) двух документов.
Наиболее известными метриками, согласно [1] (глава 3), являются следующие меры
“похожести” документов:
Коэффициент Дайса (Dice’s coefficient)
∑ ∑
∑
= =
=
+
⋅
= ⋅ n
k
j
k
n
k
i
k
n
k
j
k
i
k
i j
w w
w w
C D D
1 1
1 ( ) 2
Коэффициент Жаккара (Jaccard’s coefficient)

∑∑∑
∑
== =
=
+ − ⋅
⋅
= n
k
n
k
n
k
j
r
i
r
j
r
i
r
j
k
n
k
i
k
i j
w w w w
w w
C D D
111
1 ( )
Коэффициент косинуса
 ( )( ) ∑ ∑
∑
= =
=
⋅
⋅
= n
k
j
k
n
k
i
k
j
k
n
k
i
k
i j
w w
w w
C D D
1
2
1
2
1 ( )
В качестве стратегии поиска можно выбрать последовательный перебор - вычислять
корреляционные коэффициенты поискового запроса и каждого из исходных документов,
выбрав некоторый пороговый коэффициент корреляции cp , для вывода релевантных
документов, и в результат поискового запроса выдавать только те документы для которых
C( , DiDq ) > cp где Dq и Di векторные представления соответственно поискового запроса
21
и некоторого документа Di . Однако последовательное вычисление всех коэффициентов
корреляции между поисковым запросом и исходными документами не самый лучший
способ определения релевантных документов, в виду довольно больших вычислительных
затрат при большом количестве документов. Для преодоления этого ограничения,
похожие (в смысле корреляционного соотношения) документы на основе порогового
значения коэффициента корреляции cp группируются в кластеры, где документы более
тесно связаны друг с другом, нежели с другими кластерными группами. Далее в каждом
кластере находится центроид – типичный представитель кластера (обычно он
суррогатный и вычисляется математически), коэффициенты корреляции при нахождении
релевантных запросу документов определяются только между самим запросом и
центроидами кластеров, и если центроид релевантен запросу то можно продолжать поиск
по всем элементам данного кластера. Центроид кластера в простейшем случае можно
получить вычисляя средний элемент, описанного выше, векторного представления
документов, между всеми документами, принадлежащими кластеру, т.е. ∑=
= ⋅
m
i i
i
S
S
m
Cc
1
1 ,
где ⋅ - евклидова норма на данном векторном пространстве, а m - число всех документов
кластера. Однако построение центроида кластера, всё же довольно сложная задача, при
решении которой должна учитываться устойчивость кластеров относительно малых
модификаций документов, области их пересечения, и полнота выборки релевантных
документов. К очевидным недостаткам кластерных методов, использующих векторную
модель можно отнести довольно трудный (в основном определяемый только
эмпирическим путём) выбор параметров кластеризации, таких как функций релевантности
и их пороговых значений. Неправильный выбор данных параметров может привести к
существенным ошибкам поиска. В настоящие время кластерные методы получили
наибольшее распространения в качестве инструмента для обработки результатов
поисковых запросов, полученных с помощью методов поиска по ключевым словам,
поскольку позволяют существенно повысить точность поиска и удобочитаемость
результатов поиска.
Триангуляционные деревья
Отметим в этом разделе очень интересный метод поиска на неточное равенство в
произвольном множестве объектов, определённых в некотором пространстве с метрикой
[38]. Будем считать, что U - пространство с метрикой, содержащие конечное множество
точек, это может быть множество строк или документов, но в данном случае внутренняя
22
структура объектов здесь роли не играет. Данный метод поиска очень элегантно
использует неравенство треугольника: ∀x, y,z ∈U C(x, y) ≥ C(x,z) − C( y,z) , и строит так
называемое триангуляционное дерево (triangulation tree), позволяющее существенно
сократить количество операций сравнения в задаче поиска на неточное равенство.
Формализуем данную задачу в следующем виде: для некоторой заданной константы ε ,
точки p (эту точку можно ассоциировать с поисковым шаблоном) и множества элементов
{ }n X x x Kx 1 2 = , пространства U , необходимо найти все элементы i x в исходном
множестве X для которых будет удовлетворяться неравенство ( , ) ≤ ε i C p x . Для решения
данной задачи определим в пространстве U ещё одно множество так называемых
ключевых точек } { 1 2 m K = k k Kk , и для каждого элемента i x множества X определим
вектор его расстояний до ключевых точек ( ( , ), ( , ) ( , )) i i 1 i 2 i m v = C x k C x k KC x k . По набору
таких векторов m v ,v , ,v 1 2 K для элементов из множества X строится бинарное дерево
расстояний, где все элементы на пути от корня до листа i , образуют вектор расстояний i v
для каждого i x . В качестве примера построим триангуляционное дерево для элементов
X = {w, x, y,z}, с двумя ключевыми точками { , } 1 2 K = k k и пусть вектора расстояний до
них будут следующими: ) vw = (3,1 , ) vx = (3,1 , ) vy = (3,9 и = (4,8) z v соответственно для
каждого элемента множества X , дерево, описывающие данный случай представлено на
рисунке 4.
Рис. 4
Вернёмся к задаче поиска по множеству X на неточное соответствие некоторому
элементу p . В триангуляционном дереве по определению, для любого узла n со
значением dn , который находится на некотором уровне дерева l все элементы множества
X соответствующие листьям – потомкам данного узла, будут находится на расстоянии
dn от ключевого элемента l k . Таким образом если для некоторого узла n :
− ( , ) > ε n l d C p k , то тогда для всех листьев - потомков узла n : x X n
i ∈ , следуя
3
корень
4
1 9 8
(w,x) (y) (z)
23
определению дерева и неравенству треугольника: ( , ) ( , ) ( , )l
n
l i
n
i C p x ≥ C p k −C x k , будет
выполняться неравенство: ( , ) > ε n
i C p x , и как следствие при поиске такие узлы можно
исключить из дальнейшего рассмотрения. Проиллюстрируем поиск для примера на
рисунке 5. Пусть необходимо найти элементы близкие к элементу V с максимальным
расстоянием равным 1, и пусть ) vV = (C(V.k1 ),C(V, k2 )) = (3,8 - вектор расстояний до V .
На верхнем уровне 1, рассматриваются только узлы значениями в промежутке ] [3 −1,3 +1 ,
на рисунке они помечены звёздочкой. Далее сравниваем вторые компоненты векторов
расстояний с непосредственными потомками элементов 3 и 4, и как следствие в область
поиска попадают потомки данных узлов со значениями в промежутке ] [8 −1,8 +1 ,
спускаясь ниже определяем элементы y и z , которые соответственно находятся в шаре с
радиусом 1 возле точки поиска.
Рис 5.
Разумеется, эффективность поиска существенно зависит от выбора множества ключевых
точек в пространстве U , и если будет выбрано слишком много ключевых точек то
соответственно скорость поиска уменьшится, поскольку глубина дерева векторов
расстояний станет больше, а если таких точек будет недостаточно то снизится точность
поиска и будет велика вероятность выборки не всех элементов удовлетворяющих запросу.
3*
корень
4*
1 9* 8*
(w,x) (y)* (z)*
24
Литература
[1] C. J. van Rijsbergen."Information Retrieval". Dept. of Computer Science. University of
Glasgow, 1979.
[2] "The Deep Web: Surfacing Hidden Value". BrightPlanet LLC.
http://www.brightplanet.com/deepcontent/tutorials/DeepWeb/index.asp
[3] Sergey Brin and Lawrence Page. "The anatomy of a large-scale hypertextual Web search
engine". Computer Networks and ISDN Systems, 1998.
[4] Taher Haveliwala. "Efficient computation of pagerank". Technical Report 1999-31. Stanford
University, 1999. http://dbpubs.stanford.edu/pub/1999-31
[5] Peter Marendy. "A Review of World Wide Web searching techniques, focusing on HITS and
related algorithms that utilise the link topology of the World Wide Web to provide the basis for a
structure based search technology", 2001.
[6] Thierry Lecroq. http://www-igm.univ-mlv.fr/~lecroq/string/
[7] Graham A. Stephen. "String Search". School of Electronic Engineering Science University
College of North Wales, 1992.
[8] Леонид Бойцов. "Обзорный ресурс, посвящённый проблеме поиска по сходству".
http://itman.narod.ru
[9] Faloutsos and Oard. "A Survey of Information Retrieval and Filtering Methods". University
of Maryland
[10] Oren Zamir and Oren Etzioni. "Web Document Clustering: A Feasibility Demonstration",
1998
[11] Dawid Weiss. "Introduction to Search Results Clustering". Proceedings of the 6th
International Conference on Soft Computing and Distributed Processing. Rzeszów. Poland,
2002.
[12] Sun Wu, Udi Manber "A Tool to Search Through Entire File Systems". Proceedings of the
USENIX Winter Technical Conference, 1994.
[13] Sun Wu, Udi Manber "Agrep - A Fast Approximate Pattern-Matching Tool", 1992.
[14] Boyer R.S., Moore J.S. "A fast string searching algorithm". Communications of the ACM.
20:762-772, 1977.
[15] J. Karkkainen, G. Navarro, and E. Ukkonen. "Approximate string matching over ZivLempel compressed text", 2000.
[16] G. Navarro, R. Baeza-Yates, E. Sutinen, J. Tarhio. "Indexing Methods for Approximate
String Matching". IEEE Data Engineering Bulletin 24(4):19-27, 2001
25
[17] G. Navarro. "Approximate Text Searching" Technical Report TR/DCC-98-14, 1998
[18] D. Knuth, J. Morris, and V. Pratts. Fast pattern matching in strings. 6:322–350, 1977.
[19] Sun Wu, Udi Manber. "Fast Text Searching With Errors". Department of Computer Science
University of Arizona, 1991
[20] Luhn, H.P. "A statistical approach to mechanised encoding and searching of library
information". IBM Journal of Research and Development, 1:309-317, 1957
[21] Ashwin Ram. "Interest-based information filtering and extraction in natural language
understanding systems". Bellcore Workshop on High-Performance Information Filtering, 1991
[22] Deerwester S.C., Dumais S.T., Landauer T.K., Furnas G.W., Harshman R.A. "Indexing by
latent semantic analysis". Journal of the American Society of Information Science, 41(6):391-
407, 1990
[23] Porter M.F. "An algorithm for suffix stripping". Program, 14(3):130-137, 1980
[24] Сайт посвящённый вопросам стемминга.
http://snowball.tartarus.org/texts/introduction.html
[25] Д.Кнут Исскуство программирования Том 3.
[26] Justin Zobel, Alistair Moffat. "Inverted Files Versus Signature Files for Text Indexing".
ACM Transactions on Database Systems, 23(4):453-490, 1998
[27] Udi Manber, Gene Myers."Suffix arrays: A new method for on-line string searches".
University of Arizona, 1991
[28] M. Burrows, D.J. Wheeler. "A Block-sorting Lossless Data Compression Algorithm", 1994
[29] N. Jesper Larsson. "Notes On Suffix Sorting". 1998
[30] Kunihiko Sadakane. "A Fast Algorithm for Making Suffix Arrays and for Burrows-Wheeler
Transformation", 1998
[31] Kunihiko Sadakane. "Unifying Text Search And Compression -Suffix Sorting, Block
Sorting and Suffix Arrays-", 2000
[32] E. Ukkonen. "On-line construction of suffix trees". Algorithmica, 14:249-26, 1995
[33] Mark Nelson. "Fast String Searching With Suffix Trees". Dr. Dobb's Journal August, 1996
[34] N. Jesper Larsson. "Attack Of The Mutant Suffix Trees", 1998
[35] Moritz Maab. "Suffix Trees and their Applications", 1999
[36] "Randomized Binary Searching with Tree Structures". Communications of the ACM, March
1964
26
[37] Bentley, Sedgewick. "Fast Algorithms for Sorting and Searching Strings". SODA, 1997
[38] A. P. Berman. "A New Data Structure For Fast Approximate Matching". Technical Report 1994-03-02, Dept. of Computer Science, University of Washington, 1994
Существует шесть видов тестирования API — в зависимости от того, что именно проверяет тестировщик:

1. Тестирование методов
Каждый метод API проверяют по отдельности — чтобы убедиться, что они работают правильно и возвращают ожидаемые результаты. Тестирование включает проверку входных данных, выполнение операций и проверку вывода.

2. Тестирование взаимодействий
Проверка взаимодействия API с другими API, компонентами программы и сервисами. Показывает, что он успешно отправляет и получает данные, а также обрабатывает различные сценарии использования.

3. Тестирование авторизации и аутентификации
Проверка доступа к API: как работают механизмы авторизации, кто и к каким функциям и данным имеет доступ.

4. Тестирование обработки ошибок
Проверка поведения API в случае непредвиденных ситуаций и ошибок — например, передачи некорректных данных. Позволяет убедиться, что API правильно обрабатывает исключения и передаёт в программу верные коды ошибок.

5. Тестирование производительности
Проверка работы API при повышенных нагрузках — его пропускной способности и производительности. Позволяет убедиться, что API не «отвалится» в случае повышенного спроса на программу.

6. Тестирование безопасности
Проверка уязвимостей безопасности API, которая помогает предотвратить утечку данных или несанкционированный доступ. Проверяются меры безопасности API и проводятся тесты на проникновение для установления возможных уязвимостей.

Принципы тестирования
Чтобы создавать надёжные и эффективные тесты API, тестировщики придерживаются нескольких принципов:

Использовать правильные и разнообразные входные данные
При тестировании важно использовать разные типы данных, граничные значения, некорректные данные. Это помогает убедиться, что API правильно обрабатывает все возможные входные сценарии.

Автоматизировать тестирование
В тестировании API многое можно автоматизировать: например, проверку отдельных функций или обработку ошибок.

Проводить непрерывные тесты
Если в компании есть процессы CI/CD, важно включить в них тестирование API. Это позволит регулярно проверять его работоспособность и получать обратную связь о проблемах сразу после их возникновения.

Проверять безопасность
Поскольку API часто поставляются со стороны, важно тщательно тестировать их на уязвимости и проверять механизмы аутентификации, чтобы и API, и основная система были защищены от потенциальных угроз и атак.

На курсе по тестированию «С нуля до автоматизатора» студентов обучают этим принципам, а также всем необходимым навыкам и инструментам для тестирования API.
Инструменты тестирования API
Некоторые тесты можно проводить и вручную — так же как и программировать теоретически можно в обычном блокноте. Но на практике тестировщики используют специальные инструменты, которые позволяют упростить и автоматизировать тестирование. И делать то, что руками никак не получится.

Вот какими инструментами чаще всего пользуются тестировщики:

●	Postman. Позволяет создавать, отправлять и тестировать HTTP-запросы и получать ответы от API. Предоставляет возможность создания автоматизированных тестов, генерации документации и совместной работы в команде.
●	SoapUI. Позволяет тестировать и отлаживать SOAP и REST API: создавать и отправлять запросы, автоматизировать тестирование, генерировать тестовые отчеты, мониторить производительность.
●	JMeter. Позволяет отправлять HTTP-запросы и проводить нагрузочное тестирование, чтобы проверить, как API справляется с высокими нагрузками, насколько оно производительно и масштабируемо.
●	REST-assured. Java-библиотека, которая предоставляет простой и удобный способ для тестирования REST API с использованием DSL-синтаксиса. Она позволяет выполнять запросы, проверять ответы и создавать автоматизированные тесты.
●	Swagger. Позволяет автоматически генерировать код для тестирования API и создавать автотесты.
Шаги тестирования могут быть такими:

1. Определить требования
Изучить документацию API, чтобы понять, какие поля должны быть в задаче, как она должна быть создана и обновлена, ожидаемые коды состояния и структуру ответов.

2. Создать тестовые случаи
Например:
●	Тест случая GET /tasks — отправить GET-запрос на /tasks и убедиться, что полученный ответ содержит список задач.
●	Тест случая GET /tasks/{id} — создать тестовую задачу, получить её идентификатор, затем отправить GET-запрос на /tasks/{id} и убедиться, что возвращается информация о задаче с правильным идентификатором.
●	Тест случая POST /tasks — отправить POST-запрос на /tasks с тестовыми данными для создания новой задачи и убедиться, что задача успешно создана и возвращается правильный код состояния (например, 201 Created).
●	Тест случая PUT /tasks/{id} — создать тестовую задачу, получить её идентификатор, затем отправить PUT-запрос на /tasks/{id} с обновлёнными данными и убедиться, что задача успешно обновлена и возвращается правильный код состояния (например, 200 OK).

3. Настроить окружение
Установить Postman для отправки запросов и проверки ответов API.

4. Отправить запросы
С использованием Postman отправить запросы из тестовых случаев к API.

5. Проверить ответы
Проверить ответы от API, используя ожидаемые результаты, указанные в тестовых случаях. Например, убедиться, что возвращается правильный код состояния, структура ответа соответствует ожидаемой и значения полей задачи корректны.

6. Обработать ошибки
Проверить, как API обрабатывает ошибки, отправляя некорректные запросы. Убедиться, что возвращаются соответствующие коды и описания ошибок.

7. Сгенерировать отчёты
Создавать отчёт о результатах после каждого выполнения тестов. Включить в него информацию о прошедших и не прошедших тестах, кодах состояния, ответах, ошибках, и другие полезные данные.

8. Регулярно проводить повторное тестирование
Особенно после внесения изменений в код API или его окружения, для обнаружения проблем и подтверждения работоспособности API.
Кто такой специалист по информационной безопасности
Специалист по информационной безопасности (ИБ) — это человек, который обеспечивает сохранность данных компании. Например, клиентской базы, логинов, паролей и другой информации, хранящейся на серверах. Также специалист по ИБ может отвечать за соблюдение коммерческой тайны, тестировать приложения на уязвимость или проверять, не ведётся ли незаконная фото- и видеозапись в офисе.

Специалистов по ИБ ещё могут называть специалистами по кибербезопасности. Но в некоторых компаниях обязанности этих специалистов разделяют.
Основные обязанности специалиста по ИБ
Обязанности специалиста по информационной безопасности могут различаться в зависимости от его роли в команде и от компании, в которой он работает. Вот чем чаще всего занимаются «безопасники»:

Безопасность средств криптографической защиты

Средства криптографической защиты — это системы, которые с помощью алгоритмов криптографического преобразования зашифровывают информацию и передают её по каналам связи. К СКЗИ относится электронная подпись. Специалист по СКЗИ должен, например:

исследовать криптографические средства защиты информации и контролировать их использование в компании;
обучать сотрудников, как пользоваться СКЗИ;
расследовать нарушения в использовании таких средств.
Необходимые знания и навыки
Специалист по информационной безопасности часто работает на стыке анализа данных, разработки, юридической практики и консалтинга. В зависимости от зоны ответственности он должен обладать определёнными знаниями и навыками:

1.	Знать сетевые технологии в области обеспечения безопасности сетей, технологии контейнеризации. Контейнер — это среда, в которой можно изолированно запускать код независимо от основной системы и программного обеспечения. Для работы с контейнерами часто используют платформу Docker.

2.	Понимать, как строятся механизмы информационной безопасности операционных систем — Windows, MacOS, Linux, iOS, Android.

3.	Знать законы: например «Об информации, информационных технологиях и о защите информации».

4.	Уметь работать с системами управления базами данных.

5.	Знать языки программирования, например Java или Python.
6.	Владеть английским, чтобы читать техническую документацию.

7.	Уметь выходить за рамки. Важно, чтобы человек умел мыслить нестандартно. Это особенно поможет при поиске уязвимостей системы, когда нужно встать на место хакера.

8.	Уметь каталогизировать информацию.
На начальном этапе знание языков программирования не обязательно, но со временем придётся понять, как они устроены. Тот же Java особенно важен, если человек захочет продолжить карьеру как AppSec-инженер, тестирующий безопасность приложений.
Чем больше информации специалист соберёт, тем лучше. От этого зависит и поверхность будущей атаки. Например, сотрудник собрал все IP-адреса. Затем выяснил, какие действительны, а какие нет. Просканировал и проверил порты, узнал, какие службы на них доступны. А потом вдруг нашёл уязвимость в серверном оборудовании компании и понял, что одним нажатием может взломать сразу сто хостов. Всё благодаря тому, что потратил время на сбор этих хостов и каталогизирование.
Плюсы и минусы работы в области информационной безопасности
У специалиста по информационной безопасности широкий круг задач, поэтому ему есть куда развиваться. Это — один из плюсов профессии. Часто он работает с документацией или составляет отчёты — для кого-то такая рутина это минус. Правда, его можно компенсировать высокой зарплатой. Судя по вакансиям на hh.ru, сотрудники службы информационной безопасности получают от 100 000 ₽ в месяц.
Зарплаты в профессии
Средняя зарплата специалистов в сфере информационной безопасности в первом полугодии 2023 года составила 120 000 ₽. Это на 9% больше, чем во втором полугодии 2022 года, сообщает Хабр.Карьера.

По данным КАУС, специалисты по информационной безопасности зарабатывали в среднем 120 000—165 000 ₽ во втором квартале 2023 года. Минимальный уровень — 80 000 ₽, максимум — 500 000 ₽.

Судя по вакансиям на hh.ru, специалистам по информационной безопасности предлагают зарплату от 100 000 ₽ до 200 000 ₽ в месяц. Для пентестеров есть предложения с доходом до 300 000 ₽ в месяц.
Как стать специалистом по информационной безопасности
Стать специалистом по информационной безопасности можно как минимум двумя способами:

1.	Получить образование в вузе. В Высшей школе экономики есть направление «Информационная безопасность» — год обучения там стоит 540 000 ₽. В Московском политехническом университете стоимость годового обучения в аналогичном направлении — 301 300 ₽.

2.	Пройти онлайн-курсы. В Практикуме учат проводить тестирование на проникновение в веб-приложениях и писать безопасный код. Программа рассчитана на специалистов с опытом: разработчиков и программистов, системных администраторов и начинающих пентестеров.

Можно смотреть уроки на Youtube, читать статьи на Хабре или в блоге Практикума. Но этот вариант скорее для тех, кто хочет узнать больше о профессии и понять, подходит ли она ему.

Совет эксперта

Иван Авраменко

Если кто-то в детстве хотел быть детективом, пентест — самое то. Специалист должен знать, как работают злоумышленники, и расследовать взломы. Приходить на условное место преступления и собирать показания свидетелей. Ещё можно сравнить работу специалиста по информационной безопасности с работой в больнице. Я, когда занимаюсь пентестом, ассоциирую себя с нейрохирургом, проводящим операции на мозге. Потому что при тестировании высоконагруженных систем шаг вправо, шаг влево — и можно навредить функциональности.
Зачем нужна архитектура ПО
Чтобы создать программу, разработчики пишут код. Строки кода объединяют в функции, функции — в классы, классы — в модули. В итоге получается одна программа. Это может быть программа, с помощью которой пользователь взаимодействует с приложением, или программа, которая обрабатывает данные интернет-магазина.

Сложные IT-продукты вроде банковского приложения или корпоративного портала состоят из множества программ. Это похоже на дом, который состоит из компонентов: стены, окна, двери, мебель и техника. При этом внешне дома могут выглядеть одинаково, но внутри у каждого будет разный интерьер в зависимости от желания владельца. Так же и с разработкой ПО. Что будет внутри конечного продукта — зависит от потребностей бизнеса.

Чтобы учесть все пожелания заказчика и ускорить создание продукта, разработчики сначала создают общий план будущего программного обеспечения. То есть формируют упрощённое представление, в котором определяют основные части программы и как они будут взаимодействовать друг с другом и внешним миром. Например, с платёжными системами или социальными сетями через API. Это похоже на создание общего чертежа здания, в котором пока не проработаны детали каждого этажа и комнаты, а только главные блоки и их соединения.

Далее разработчики детализируют «чертёж» — прорабатывают конкретные технические характеристики и требования. В результате у команды появляется чёткое представление, какую программу нужно создать: какие функции она должна выполнять, какие данные будут обрабатываться и какой результат должен получаться на выходе у каждой её части. Другими словами, разработчики получают полную информацию для построения всех компонентов программы.
Обычно архитектуру разрабатывают опытные программисты, или архитекторы ПО. Они могут подобрать решение, провести эксперименты, чтобы проверить его, а потом защитить перед командой и заказчиками. Но кроме знания языков программирования, принципов работы баз данных, компьютерных и операционных систем для этого нужны умения разбираться в задаче и работать в команде. На курсах Практикума студенты не только учат теорию и осваивают инструменты, но и тренируют навыки, которые пригодятся для карьерного роста.
Обзор архитектур и их характеристики
Архитектурных решений очень много, как и их классификаций. Универсальных шаблонов не существует. Выбор вида архитектуры программного обеспечения зависит от задачи: целей продукта, требований и ресурсов заказчика — временных и материальных.

В статье рассмотрим два основных подхода к проектированию архитектуры компонентов программного обеспечения или системы в целом.

1. Монолитный подход
Это традиционный подход к разработке программного обеспечения. В монолитной архитектуре все функции тесно связаны и управляются как единое целое. Это удобно для небольших или средних проектов: можно быстро развернуть ПО.

При совместной разработке или рефакторинге, например когда проект растёт, с монолитной архитектурой могут возникнуть проблемы. Если над созданием ПО работает большая команда, интеграция изменений от разных разработчиков может вызвать конфликты версий. Одна ошибка при фиксации изменений в системе контроля версий нарушит работу всего ПО.

При обновлении монолитных систем есть риск сбоев, поэтому их нужно тщательнее тестировать. Но тестирование усложняется с увеличением размеров кодовой базы. Полный комплекс тестов может занять много времени, что замедлит процесс разработки.

Монолитные системы сложно масштабировать. Все компоненты приложения нагружают один и тот же сервер или кластер. Если нужно увеличить ресурсы для одной части системы, придётся увеличивать ресурсы для всей системы. Это не всегда экономически оправдано.
2. Микросервисный подход
Программа разбивается на отдельные сервисы, каждый из которых выполняет свою функцию. Например, приложение для интернет-магазина может состоять из сервисов для каталога товаров, оформления заказов, обработки платежей и авторизации пользователей в личном кабинете.

Микросервисный подход к организации работы системы принято считать более гибким. Можно по отдельности масштабировать или изменять сервисы, например добавлять новые функции. Изменения одного сервиса никак не повлияют на работу остальных. Если потребуется переписать один из сервисов на другом языке программирования — просто сохраняется контракт, по которому компоненты взаимодействуют.

Множество компонентов системы взаимодействуют между собой по сети, поэтому на их совместную работу слабо влияет, на каком количестве компьютеров они запущены. Благодаря этому микросервисные системы относительно легко масштабировать. Однако нужны дополнительные ресурсы и инструменты, чтобы реализовать взаимодействие между компонентами, обеспечивать согласованность данных и поддерживать контракты.
Технические характеристики микросервисных и монолитных архитектур не стоит разделять на преимущества и недостатки. Выбор решения зависит от множества факторов. Важно учитывать текущие и будущие потребности бизнеса, ресурсы команды, уровень зрелости и способность IT-инфраструктуры компании поддерживать выбранный подход. Большую роль играют стратегическое планирование и способность команды адаптироваться к процессам разработки и поддержки.

О чём помнить при создании архитектуры ПО
Нет пошаговых алгоритмов для выбора подхода к разработке ПО. В реальности может быть два почти одинаковых техзадания, которые отличаются всего несколькими словами, например, с каким объёмом данных нужно работать, сколько в команде человек, какой бюджет или сроки. Но из-за этих отличий архитектурные решения будут абсолютно разные.

Вот две рекомендации, о которых стоит помнить на практике:

1. Работа над архитектурным решением должна начинаться с исследования.

Вернёмся к нашему примеру со строительством дома. У двух одинаковых домов на соседних улицах может быть совершенно разный фундамент. На выбор типа фундамента влияют, например, особенности почвы и высота грунтовых вод. Поэтому сначала нужно исследовать землю, на которой будут строить здание. Так же и с разработкой архитектурного решения.

Перед выбором подхода нужно изучить требования заказчика, продукт, пользователей, технические мощности компании, ожидаемую нагрузку, затраты на начальном этапе и в перспективе для поддержки и масштабирования системы. И многие другие факторы.
2. Необязательно создавать всю архитектуру программного обеспечения по одному принципу.

Микросервисный и монолитный подходы — это всего лишь инструменты. Какой из них использовать, зависит от конкретного случая. При этом их можно и нужно комбинировать, если это поможет эффективнее решать задачи.

Например, в корпоративном портале компонент, который формирует отчёты, можно написать монолитом, а модуль для передачи данных между пользователями — микросервисно. Один микросервис будет загружать данные, второй — обрабатывать их, третий — отправлять данные в монолитный компонент на формирование счетов и отчётов.

Если пытаться сделать всю архитектуру ПО монолитно или микросервисно, продукт может перестать работать так, как требуется. Тогда придётся заново проектировать решение и переписывать код.
Кто такой архитектор программного обеспечения
Архитектор ПО — это скорее должность или роль в компании, чем профессия в общепринятом понимании. Таких специалистов не готовят в вузах, в России нет ни одного человека с дипломом этого профиля, а в общероссийском классификаторе профессий и должностей (ОКПДТР) архитектор ПО не значится.
Архитектор ПО — это опытный программист, который может не только написать код, но и глубоко вникнуть в идею заказчика, придумать, как её лучше реализовать с технической точки зрения, и спроектировать будущую программу с нуля. В разработке ПО архитектор — что-то между инженером, дизайнером и проектным менеджером, который много взаимодействует с командой и заказчиком и отвечает за успех всего проекта.
Что именно делает архитектор ПО, проще объяснить на примере. Когда нужно написать небольшую программу, например калькулятор ипотеки, разработчик может справиться в одиночку: обсудить детали проекта напрямую с заказчиком и за несколько дней написать программу. Для разработки более масштабных проектов, например банковского приложения, понадобится от нескольких месяцев до года и команда специалистов — UI/UX дизайнеры, разработчики, тестировщики. Каждый отвечает за свою часть, поэтому кто-то должен управлять всем процессом — собирать пожелания заказчика и продумывать, как их реализовать, разбивать проект на подзадачи и раздавать их исполнителям. Такого человека называют архитектором ПО.
Дмитрий Орлов, технический менеджер проектов
Чтобы разработать информационную систему с множеством разных сервисов, например для банка, нужны топовые IT-архитекторы. Из-за того, что таких специалистов мало, появились «архитекторы решений» — Solution Architect. Это специалист высшего уровня, который способен разработать систему от А до Я, с опытом работы в крупных компаниях и уже реализованными проектами. Он легко разбирается в запросах бизнеса и проектирует систему — при этом за реализацию проекта могут отвечать другие специалисты.
Набор конкретных знаний, инструментов и навыков, которыми должен владеть архитектор ПО, зависит от компании и проекта. В крупных компаниях, например в Сбере или Яндексе, — масштабные проекты и сложные задачи, с которыми специалисты с небольшим опытом могут не справиться. Но в компаниях поменьше IT-архитекторы тоже нужны: например, чтобы интегрировать информационную систему сети ресторанов с Яндекс Едой или связать 1С небольшого производства с системой продаж Ozon. С подобными задачами вполне способны справиться даже новички в архитектуре. Достаточно понимать, как работают популярные базы данных, как разрабатывать и поддерживать системы заказчика, и иметь опыт разработки.
Как стать архитектором программного обеспечения
Ворваться в профессию «с нуля» не получится. Чтобы проектировать ПО, нужен большой опыт в разработке — фронтенд, бэкенд или фулстек, а также высокая квалификация и широкий кругозор. Человек, который задумывается о профессии архитектора ПО, должен прекрасно знать минимум один, а лучше несколько языков программирования, уметь писать и читать код, а также на базовом уровне знать, как работают компьютеры и распределённые системы.
На любом собеседовании в крупной компании разработчики проходят так называемую «архитектурную секцию» — HR-специалисты выясняют, способен ли кандидат решать задачи архитектора ПО. Если секция успешно пройдена, значит, у специалиста высокий профессиональный уровень и в будущем ему могут предложить должность архитектора ПО. Если ждать повышения не хочется, можно получить недостающие знания — самостоятельно или на курсах — и начать откликаться на вакансии по профилю.
Что такое микроразметка
Микроразметка — это добавление в существующий HTML-код страницы элементов языка — тегов и атрибутов, которые помогают поисковым ботам распознавать контент и показывать пользователям то, что они ищут. Например, по запросу «рецепт шоколадного печенья», поисковик сразу выдаст список рецептов с фотографиями и необходимой информацией.
Если разработчик выполнит все настройки верно, то поисковые системы считают содержимое веб-страницы и сгенерируют визуально приятный сниппет с понятным предложением. Без разметки в поисковике отобразится минимум информации: обычно это URL-адрес сайта и простое однострочное описание. Сколько данных разместить в выдаче — решает специалист.
Семантическая разметка нужна, чтобы сделать поиск в интернете удобнее. Она помогает специальным программам извлекать и обрабатывать информацию и делать читабельной и понятной для человека.
Научиться настраивать и проверять микроразметку можно на курсе «Веб-разработчик». На каждом этапе пути студентов поддерживает команда профессиональных ревьюеров, кураторов и наставников — это помогает лучше усвоить материал и подготовиться к работе по новой специальности.